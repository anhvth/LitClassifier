{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d2c017-7c7a-48b0-ac7f-8c19c7074e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp lit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34991d1-2a84-4202-a436-f840e06e2166",
   "metadata": {},
   "source": [
    "# LitModel\n",
    "> Lit model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69641cf-7889-43c6-89e7-1cde9d864950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37a026-d840-4e4e-a449-ee5e38698326",
   "metadata": {},
   "source": [
    "# LIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994e324a-8c1e-4a63-b26e-158515708517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf2cdb5f-da74-4702-a4ee-802c67d8bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "DEFAULT_HPARAMS = {\n",
    "    \"num_workers\": 12,\n",
    "    \"image_size\": 64,\n",
    "    \"lr\": 2e-3,\n",
    "    \"dropout\": 0.2,\n",
    "    \"max_epochs\": 100,\n",
    "    \"init_lr\": 8e-5,\n",
    "    \"num_training_steps\": 2000,\n",
    "}\n",
    "\n",
    "class LitModel(LightningModule):\n",
    "    def __init__(self, model, num_classes=None,hparams=DEFAULT_HPARAMS):\n",
    "        super().__init__()\n",
    "        if isinstance(model, str):\n",
    "            assert num_classes is not None, 'num_classes cannot be none with model={}'.format(model)\n",
    "            self.model = model_factory(model, num_classes)\n",
    "        elif isinstance(model, torch.nn.Module):\n",
    "            self.model = model\n",
    "\n",
    "        self.loss_fn = FocalLoss()\n",
    "        self._hparams = hparams\n",
    "\n",
    "\n",
    "    def get_linear_schedule_with_warmup(self,\n",
    "        optimizer, num_warmup_steps, num_training_steps, init_lr=5e-4, last_epoch=-1\n",
    "    ):  \n",
    "        from torch.optim.lr_scheduler import LambdaLR\n",
    "        def lr_lambda(current_step: int):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps)) + init_lr\n",
    "            return max(\n",
    "                0.0,\n",
    "                float(num_training_steps - current_step)\n",
    "                / float(max(1, num_training_steps - num_warmup_steps)),\n",
    "            )\n",
    "\n",
    "        return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self._hparams[\"lr\"])\n",
    "\n",
    "        scheduler = {\n",
    "            \"scheduler\": self.get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                self._hparams[\"num_training_steps\"] * 0.15,\n",
    "                self._hparams[\"num_training_steps\"],\n",
    "                self._hparams[\"init_lr\"],\n",
    "            ),\n",
    "            \"interval\": \"step\",  # or 'epoch'\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        logits = self(x)[0]\n",
    "        scores = logits.sigmoid()\n",
    "        # return dict(scores=scores)\n",
    "        return scores\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x)[0]\n",
    "        loss = self.loss_fn(logits, y.view(len(logits),))\n",
    "\n",
    "        preds = logits.sigmoid().argmax(1)\n",
    "        accs = (y == preds).float().mean()\n",
    "\n",
    "\n",
    "        self.log(\"val_loss\", loss, rank_zero_only=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", accs, rank_zero_only=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x)[0]\n",
    "        loss = self.loss_fn(logits, y.view(len(logits),))\n",
    "\n",
    "        preds = logits.sigmoid().argmax(1)\n",
    "        accs = (y == preds).float().mean()\n",
    "        \n",
    "        self.log(\"training_loss\", loss, prog_bar=True, rank_zero_only=True)\n",
    "        self.log(\"training_accuracy\", accs, prog_bar=True, rank_zero_only=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def get_trainer(exp_name, gpus=1, max_epochs=40,\n",
    "                distributed=False, trainer_strategy='ddp',\n",
    "                monitor=dict(metric=\"val_loss\", mode=\"min\"), save_every_n_epochs=3, \n",
    "                ):\n",
    "\n",
    "\n",
    "    now = datetime.now() + timedelta(hours=7)\n",
    "    root_log_dir = osp.join(\n",
    "            \"lightning_logs\", exp_name, now.strftime(\n",
    "                \"%b%d-%H:%M:%S\")\n",
    "        )\n",
    "    filename=\"{epoch}-{\"+monitor[\"metric\"]+\":.2f}\"\n",
    "\n",
    "    callback_ckpt = ModelCheckpoint(\n",
    "        dirpath=osp.join(root_log_dir, \"ckpts\"),\n",
    "        monitor=monitor['metric'],\n",
    "        filename=filename,\n",
    "        mode=monitor['mode'],\n",
    "        save_last=True,\n",
    "        every_n_epochs=save_every_n_epochs,\n",
    "    )\n",
    "\n",
    "    callback_tqdm = TQDMProgressBar(refresh_rate=5)\n",
    "    callback_lrmornitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    plt_logger = TensorBoardLogger(\n",
    "        osp.join(root_log_dir, \"tb_logs\"), version=now.strftime(\"%b%d-%H:%M:%S\")\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        gpus=gpus,\n",
    "        max_epochs=max_epochs,\n",
    "        strategy=trainer_strategy,# if distributed else 'dp',\n",
    "        callbacks=[callback_ckpt, callback_tqdm, callback_lrmornitor],\n",
    "        logger=plt_logger,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ee2f3-7c66-4a6d-8ef3-d2900ffab513",
   "metadata": {},
   "source": [
    "# Mnist example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e9f882e-fd65-4f24-8277-f87e13724232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e192f72-c76d-4852-9ebe-8abe23b2a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_rgb_lambda = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74ff5c68-3ebf-49f3-8991-3e370051c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init DataLoader from MNIST Dataset\n",
    "train_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.Compose(\n",
    "[    transforms.Lambda(lambda x: x.convert('RGB')),\n",
    "    transforms.ToTensor(),]\n",
    "))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2b8afc8-6ff2-4864-b190-b83df3f4f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lit_classifier import base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee3d6fe2-c3db-4eca-8dc1-a635f894907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b014da9e-9557-41c3-a4ef-771479382f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0b96393-0079-4340-9235-a627ff61c643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 11:26:20.534 | INFO     | lit_classifier.base_model:model_factory:91 - Init model mobilenetv2_100\n"
     ]
    }
   ],
   "source": [
    "model = base_model.model_factory('mobilenetv2_100', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4916b454-ff6f-4763-ae5b-46504b419974",
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5m/vb0j6jw10zg1v3ff4d677znh0000gp/T/ipykernel_17275/951958078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/5m/vb0j6jw10zg1v3ff4d677znh0000gp/T/ipykernel_17275/3702843322.py\u001b[0m in \u001b[0;36mget_trainer\u001b[0;34m(exp_name, gpus, max_epochs, distributed, trainer_strategy, monitor, save_every_n_epochs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     trainer = Trainer(\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;31m# all args were already moved to kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, num_processes, tpu_cores, ipus, gpus, gpu_ids)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# 6. Instantiate Strategy - Part 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_init_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_deterministic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py\u001b[0m in \u001b[0;36m_lazy_init_strategy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_IS_INTERACTIVE\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_interactive_compatible\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m             raise MisconfigurationException(\n\u001b[0m\u001b[1;32m    761\u001b[0m                 \u001b[0;34mf\"`Trainer(strategy={self.strategy.strategy_name!r})` or\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0;34mf\" `Trainer(accelerator={self.strategy.strategy_name!r})` is not compatible with an interactive\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` or `Trainer(accelerator='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: Trainer(strategy=None|dp|tpu_spawn). In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "trainer = get_trainer('test', gpus=None, trainer_strategy='dp', distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74ed0279-2f3e-4cb6-85f3-1a2583f2ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8076127-e0a9-42b5-a7ae-a2da3e3ef259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
