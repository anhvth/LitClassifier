{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d2c017-7c7a-48b0-ac7f-8c19c7074e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dnnlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327322c-cde3-48b9-b345-7a822fcbff95",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d37a026-d840-4e4e-a449-ee5e38698326",
   "metadata": {},
   "source": [
    "# LIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "994e324a-8c1e-4a63-b26e-158515708517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import torch\n",
    "from datetime import datetime, timedelta\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import os\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "import os.path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf2cdb5f-da74-4702-a4ee-802c67d8bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_HPARAMS = {\n",
    "    \"num_workers\": 12,\n",
    "    \"image_size\": 64,\n",
    "    \"lr\": 2e-3,\n",
    "    \"dropout\": 0.2,\n",
    "    \"max_epochs\": 100,\n",
    "    \"init_lr\": 8e-5,\n",
    "    \"num_training_steps\": 2000,\n",
    "}\n",
    "\n",
    "class LitModel(LightningModule):\n",
    "    def __init__(self, model, num_classes=None,hparams=DEFAULT_HPARAMS):\n",
    "        super().__init__()\n",
    "        if isinstance(model, str):\n",
    "            assert num_classes is not None, 'num_classes cannot be none with model={}'.format(model)\n",
    "            self.model = model_factory(model, num_classes)\n",
    "        elif isinstance(model, torch.nn.Module):\n",
    "            self.model = model\n",
    "\n",
    "        self.loss_fn = FocalLoss()\n",
    "        self._hparams = hparams\n",
    "\n",
    "\n",
    "    def get_linear_schedule_with_warmup(self,\n",
    "        optimizer, num_warmup_steps, num_training_steps, init_lr=5e-4, last_epoch=-1\n",
    "    ):  \n",
    "        from torch.optim.lr_scheduler import LambdaLR\n",
    "        def lr_lambda(current_step: int):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps)) + init_lr\n",
    "            return max(\n",
    "                0.0,\n",
    "                float(num_training_steps - current_step)\n",
    "                / float(max(1, num_training_steps - num_warmup_steps)),\n",
    "            )\n",
    "\n",
    "        return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self._hparams[\"lr\"])\n",
    "\n",
    "        scheduler = {\n",
    "            \"scheduler\": self.get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                self._hparams[\"num_training_steps\"] * 0.15,\n",
    "                self._hparams[\"num_training_steps\"],\n",
    "                self._hparams[\"init_lr\"],\n",
    "            ),\n",
    "            \"interval\": \"step\",  # or 'epoch'\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        logits = self(x)[0]\n",
    "        scores = logits.sigmoid()\n",
    "        # return dict(scores=scores)\n",
    "        return scores\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x)[0]\n",
    "        loss = self.loss_fn(logits, y.view(len(logits),))\n",
    "\n",
    "        preds = logits.sigmoid().argmax(1)\n",
    "        accs = (y == preds).float().mean()\n",
    "\n",
    "\n",
    "        self.log(\"val_loss\", loss, rank_zero_only=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", accs, rank_zero_only=True,\n",
    "                    on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch[:2]\n",
    "        logits = self(x)[0]\n",
    "        loss = self.loss_fn(logits, y.view(len(logits),))\n",
    "\n",
    "        preds = logits.sigmoid().argmax(1)\n",
    "        accs = (y == preds).float().mean()\n",
    "        \n",
    "        self.log(\"training_loss\", loss, prog_bar=True, rank_zero_only=True)\n",
    "        self.log(\"training_accuracy\", accs, prog_bar=True, rank_zero_only=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def get_trainer(exp_name, gpus=1, max_epochs=40,\n",
    "                distributed=False, trainer_strategy='ddp',\n",
    "                monitor=dict(metric=\"val_loss\", mode=\"min\"), save_every_n_epochs=3, \n",
    "                ):\n",
    "\n",
    "\n",
    "    now = datetime.now() + timedelta(hours=7)\n",
    "    root_log_dir = osp.join(\n",
    "            \"lightning_logs\", exp_name, now.strftime(\n",
    "                \"%b%d-%H:%M:%S\")\n",
    "        )\n",
    "    filename=\"{epoch}-{\"+monitor[\"metric\"]+\":.2f}\"\n",
    "\n",
    "    callback_ckpt = ModelCheckpoint(\n",
    "        dirpath=osp.join(root_log_dir, \"ckpts\"),\n",
    "        monitor=monitor['metric'],\n",
    "        filename=filename,\n",
    "        mode=monitor['mode'],\n",
    "        save_last=True,\n",
    "        every_n_epochs=save_every_n_epochs,\n",
    "    )\n",
    "\n",
    "    callback_tqdm = TQDMProgressBar(refresh_rate=5)\n",
    "    callback_lrmornitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "    plt_logger = TensorBoardLogger(\n",
    "        osp.join(root_log_dir, \"tb_logs\"), version=now.strftime(\"%b%d-%H:%M:%S\")\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        gpus=gpus,\n",
    "        max_epochs=max_epochs,\n",
    "        strategy=trainer_strategy if distributed else 'dp',\n",
    "        callbacks=[callback_ckpt, callback_tqdm, callback_lrmornitor],\n",
    "        logger=plt_logger,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ee2f3-7c66-4a6d-8ef3-d2900ffab513",
   "metadata": {},
   "source": [
    "# Mnist example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c601532-814a-4d7d-8c42-a936b8f34de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ff5c68-3ebf-49f3-8991-3e370051c9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: VG-Blockpage\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)>\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error downloading train-images-idx3-ubyte.gz",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Init DataLoader from MNIST Dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_DATASETS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE)\n",
      "File \u001b[0;32m~/.conda/envs/dms/lib/python3.8/site-packages/torchvision/datasets/mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/dms/lib/python3.8/site-packages/torchvision/datasets/mnist.py:195\u001b[0m, in \u001b[0;36mMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError downloading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error downloading train-images-idx3-ubyte.gz"
     ]
    }
   ],
   "source": [
    "# Init DataLoader from MNIST Dataset\n",
    "train_ds = MNIST(PATH_DATASETS, train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b475bcf-d905-4199-a403-05f60da7401a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anhvth8/gitprojects/LitClassifier/nbs'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031857ac-d2af-46ab-b9ad-64c4d89ce1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wget-rs \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\" -o "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
